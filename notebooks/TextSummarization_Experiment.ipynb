{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module includes logging configurations\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from from_root import from_root\n",
    "\n",
    "\n",
    "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "LOG_DIR = 'logs'\n",
    "\n",
    "logs_path = os.path.join(LOG_DIR, LOG_FILE)\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=logs_path,\n",
    "    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module includes codes that defines custom Exception\"\"\"\n",
    "\n",
    "import types\n",
    "\n",
    "\n",
    "def error_message_detail(error, error_detail: types.ModuleType):\n",
    "    \"\"\"This function is used to\"\"\"\n",
    "    _, _, exc_tb = error_detail.exc_info()\n",
    "\n",
    "    file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "\n",
    "    error_message = f\"Error occurred python script name {file_name} \\\n",
    "    line number {exc_tb.tb_lineno} error message {str(error)}\"\n",
    "\n",
    "    return error_message\n",
    "\n",
    "\n",
    "class TextSummarizerException(Exception):\n",
    "    \"\"\"This class encapsulated the method that returns error message\"\"\"\n",
    "\n",
    "    def __init__(self, error_message, error_detail):\n",
    "        \"\"\"\n",
    "        :param error_message: error message in string format\n",
    "        \"\"\"\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = error_message_detail(\n",
    "            error_message, error_detail=error_detail\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module maps the configuration for all the constants in each pipeline\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "TIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\n",
    "ARTIFACTS_ROOT: str = os.path.join(\"artifacts\",TIMESTAMP)\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "  NUM_TRAIN_EPOCHS = 1\n",
    "  WARMUP_STEPS = 500\n",
    "  PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "  PER_DEVICE_EVAL_BATCH_SIZE = 1\n",
    "  WEIGHT_DECAY = 0.01\n",
    "  LOGGING_STEPS = 10\n",
    "  EVALUATION_STRATEGY = \"steps\"\n",
    "  EVAL_STEPS = 50\n",
    "  SAVE_STEPS = 1e6\n",
    "  GRADIENT_ACCUMULATION_STEPS = 16\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConstants:\n",
    "  DATA_INGESTION_ROOT_DIR: str = os.path.join(ARTIFACTS_ROOT,\"DataIngestionArtifacts\")\n",
    "  DATA_FILE_NAME: str = \"data.zip\"\n",
    "  DATA_URL: str = \"https://text-summer-bucket.s3.amazonaws.com/summarizer-data.zip\"\n",
    "  DOWNLOADED_DATA_FILE: str = os.path.join(DATA_INGESTION_ROOT_DIR, DATA_FILE_NAME)\n",
    "  UNZIPPED_DIR: str =  DATA_INGESTION_ROOT_DIR\n",
    "  DATA_BUCKET_NAME: str = \"text-summarization-data-06062024\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataValidationConstants:\n",
    "  DATA_VALIDATION_STATUS_FILE = \"status.txt\"\n",
    "  ALL_REQUIRED_FILES: List[str] = field(default_factory=list)\n",
    "  DATA_VALIDATION_ROOT_DIR: str = os.path.join(ARTIFACTS_ROOT,\"DataValidationArtifacts\")\n",
    "  DATA_VALIDATION_STATUS_FILE: str = os.path.join(DATA_VALIDATION_ROOT_DIR, DATA_VALIDATION_STATUS_FILE)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConstants:\n",
    "  DATA_TRANSFORMATION_ROOT_DIR: str = os.path.join(ARTIFACTS_ROOT,\"DataTransformationArtifacts\")\n",
    "  TRANSFORMED_DATA_PATH: str = DataIngestionConstants.DATA_INGESTION_ROOT_DIR\n",
    "  TOKENIZER_NAME: str = \"google/pegasus-cnn_dailymail\"\n",
    "  MAX_INPUT_LENGTH: int = 1024\n",
    "  MAX_TARGET_LENGTH: int = 128\n",
    "  PREFIX: str = \"Summarize: \"\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingConstants:\n",
    "  MODEL_TRAINING_ROOT_DIR: str = os.path.join(ARTIFACTS_ROOT, \"ModelTraining\")\n",
    "  MODEL_TRAINING_DATA_PATH: str = DataTransformationConstants.DATA_TRANSFORMATION_ROOT_DIR\n",
    "  MODEL_CKPT: str = \"google/pegasus-cnn_dailymail\"\n",
    "  MODEL_PATH: str = os.path.join(MODEL_TRAINING_ROOT_DIR, \"TrainedModel\")\n",
    "  TOKENIZER_PATH: str = os.path.join(MODEL_TRAINING_ROOT_DIR, \"Tokenizer\")\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConstants:\n",
    "  MODEL_EVALUATION_ROOT_DIR: str = os.path.join(ARTIFACTS_ROOT, \"ModelEvaluation\")\n",
    "  DATA_PATH: str =  ModelTrainingConstants.MODEL_TRAINING_DATA_PATH\n",
    "  SAVED_MODEL_PATH: str = ModelTrainingConstants.MODEL_PATH\n",
    "  TOKENIZER_PATH: str =  ModelTrainingConstants.TOKENIZER_PATH\n",
    "  METRIC_FILE_NAME: str = os.path.join(MODEL_EVALUATION_ROOT_DIR, \"metrics.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module includes all the configurations for each stage of pipeline\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    data_url: str\n",
    "    downloaded_data_file: Path\n",
    "    unzipped_dir: Path\n",
    "    data_bucket_name: str\n",
    "    data_file_name: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    status_file: str\n",
    "    all_required_files: list\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    transformed_data_path: Path\n",
    "    tokenizer_name: Path\n",
    "    max_input_length: int\n",
    "    max_target_length: int\n",
    "    prefix: str\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    saved_model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
    "    \"\"\"reads yaml file and returns\n",
    "\n",
    "    Args:\n",
    "        path_to_yaml (str): path like input\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if yaml file is empty\n",
    "        e: empty file\n",
    "\n",
    "    Returns:\n",
    "        ConfigBox: ConfigBox type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_yaml) as yaml_file:\n",
    "            content = yaml.safe_load(yaml_file)\n",
    "            logging.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
    "            return ConfigBox(content)\n",
    "    except BoxValueError:\n",
    "        raise ValueError(\"yaml file is empty\")\n",
    "    except Exception as error:\n",
    "        logging.error(error)\n",
    "        raise error\n",
    "    \n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def create_directories(list_of_directories: list, verbose=True):\n",
    "    \"\"\"create list of directories\n",
    "\n",
    "    Args:\n",
    "        path_to_directories (list): list of path of directories\n",
    "        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n",
    "    \"\"\"\n",
    "    for dir in list_of_directories:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        if verbose:\n",
    "            logging.info(f\"created directory at: {dir}\")\n",
    "\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def get_size(file_path: Path) -> str:\n",
    "    \"\"\"get size in KB\n",
    "\n",
    "    Args:\n",
    "        path (Path): path of the file\n",
    "\n",
    "    Returns:\n",
    "        str: size in KB\n",
    "    \"\"\"\n",
    "    size_in_kb = round(os.path.getsize(file_path)/1024)\n",
    "    return f\"~ {size_in_kb} KB\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module basically manages the configurations for each stage fo the pipeline\n",
    "\"\"\"\n",
    "\n",
    "# import TrainingArguments, DataIngestionConstants, DataTransformationConstants, DataValidationConstants, ModelTrainingConstants, ModelEvaluationConstants\n",
    "\n",
    "# from src.text_summarization.utils.common_utils import read_yaml, create_directories\n",
    "\n",
    "# import DataIngestionConfig, DataValidationConfig, DataTransformationConfig, ModelTrainingConfig, ModelEvaluationConfig\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"This class binds the methods for all the configuration files\"\"\"\n",
    "    def __init__(self):\n",
    "        self.data_ingestion_const = DataIngestionConstants()\n",
    "        self.data_validation_const = DataValidationConstants()\n",
    "        self.data_transformation_const = DataTransformationConstants()\n",
    "        self.model_training_const = ModelTrainingConstants()\n",
    "        self.model_evaluation_const = ModelEvaluationConstants()\n",
    "        \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"This method assigns the constants for Data Ingestion config\"\"\"\n",
    "\n",
    "        create_directories([self.data_ingestion_const.DATA_INGESTION_ROOT_DIR])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir = self.data_ingestion_const.DATA_INGESTION_ROOT_DIR,\n",
    "            data_url = self.data_ingestion_const.DATA_URL,\n",
    "            downloaded_data_file = self.data_ingestion_const.DOWNLOADED_DATA_FILE,\n",
    "            unzipped_dir = self.data_ingestion_const.UNZIPPED_DIR,\n",
    "            data_bucket_name = self.data_ingestion_const.DATA_BUCKET_NAME,\n",
    "            data_file_name = self.data_ingestion_const.DATA_FILE_NAME\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "    \n",
    "  \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"This method assigns the constants for Data Validation config\"\"\"\n",
    "\n",
    "        create_directories([self.data_validation_const.DATA_VALIDATION_ROOT_DIR])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir = self.data_validation_const.DATA_VALIDATION_ROOT_DIR,\n",
    "            status_file = self.data_validation_const.DATA_VALIDATION_STATUS_FILE,\n",
    "            all_required_files = [\"samsum-train.csv\", \"samsum-test.csv\", \"samsum-validation.csv\"]\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "    \n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"This method assigns the constants for Data Transformation config\"\"\"\n",
    "\n",
    "        create_directories([self.data_transformation_const.DATA_TRANSFORMATION_ROOT_DIR])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = self.data_transformation_const.DATA_TRANSFORMATION_ROOT_DIR,\n",
    "            transformed_data_path = self.data_transformation_const.TRANSFORMED_DATA_PATH,\n",
    "            tokenizer_name = self.data_transformation_const.TOKENIZER_NAME,\n",
    "            max_input_length= self.data_transformation_const.MAX_INPUT_LENGTH,\n",
    "            max_target_length= self.data_transformation_const.MAX_TARGET_LENGTH,\n",
    "            prefix = self.data_transformation_const.PREFIX\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "    \n",
    "\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        \"\"\"This method assigns the constants for Model Training config\"\"\"\n",
    "\n",
    "        config = ModelTrainingConstants()\n",
    "        params = TrainingArguments()\n",
    "        create_directories([config.MODEL_TRAINING_ROOT_DIR])\n",
    "\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir = config.MODEL_TRAINING_ROOT_DIR,\n",
    "            data_path=config.MODEL_TRAINING_DATA_PATH,\n",
    "            model_ckpt = config.MODEL_CKPT,\n",
    "            model_path = config.MODEL_PATH,\n",
    "            tokenizer_path = config.TOKENIZER_PATH,\n",
    "            num_train_epochs = params.NUM_TRAIN_EPOCHS,\n",
    "            warmup_steps = params.WARMUP_STEPS,\n",
    "            per_device_train_batch_size = params.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            per_device_eval_batch_size = params.PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "            weight_decay = params.WEIGHT_DECAY,\n",
    "            logging_steps = params.LOGGING_STEPS,\n",
    "            evaluation_strategy = params.EVALUATION_STRATEGY,\n",
    "            eval_steps = params.EVAL_STEPS,\n",
    "            save_steps = params.SAVE_STEPS,\n",
    "            gradient_accumulation_steps = params.GRADIENT_ACCUMULATION_STEPS\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"This method assigns the constants for Model Evaluation config\"\"\"\n",
    "\n",
    "        config = ModelEvaluationConstants()\n",
    "        \n",
    "        create_directories([config.MODEL_EVALUATION_ROOT_DIR])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.MODEL_EVALUATION_ROOT_DIR,\n",
    "            data_path=config.DATA_PATH,\n",
    "            model_path = config.SAVED_MODEL_PATH,\n",
    "            tokenizer_path = config.TOKENIZER_PATH,\n",
    "            metric_file_name = config.METRIC_FILE_NAME\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "import pickle\n",
    "from pandas import DataFrame, read_csv\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from mypy_boto3_s3.service_resource import Bucket\n",
    "\n",
    "class S3Operations:\n",
    "    \"\"\"This class encapsulates contains all the methods that will be used for S3 Operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.s3_client = boto3.client(\"s3\")\n",
    "        self.s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "    def download_object(self, file_name, bucket_name, file_path):\n",
    "        \"\"\"This method is used for downloading the file from S3\"\"\"\n",
    "        bucket = self.s3_resource.Bucket(bucket_name)\n",
    "        bucket.download_file(Key=file_name, Filename=file_path)\n",
    "\n",
    "\n",
    "    def get_bucket(self, bucket_name: str) -> Bucket:\n",
    "        \"\"\"\n",
    "        Method Name :   get_bucket\n",
    "        Description :   This method gets the bucket object based on the bucket_name\n",
    "        Output      :   Bucket object is returned based on the bucket name\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the get_bucket method of S3Operations class\")\n",
    "        try:\n",
    "            bucket = self.s3_resource.Bucket(bucket_name)\n",
    "            logging.info(\"Completed execution of the get_bucket method of S3Operations class\")\n",
    "            return bucket\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def is_model_present(self, bucket_name: str, s3_model_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   is_model_present\n",
    "        Description :   This method validates whether model is present in the s3 bucket or not.\n",
    "        Output      :   True or False\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.get_bucket(bucket_name)\n",
    "\n",
    "            return any(True for _ in bucket.objects.filter(Prefix=s3_model_key))\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def get_file_object(self, filename: str, bucket_name: str) -> Union[List[object], object]:\n",
    "        \"\"\"\n",
    "        Method Name :   get_file_object\n",
    "        Description :   This method gets the file object from bucket_name bucket based on filename\n",
    "        Output      :   list of objects or object is returned based on filename\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the get_file_object method of S3Operations class\")\n",
    "        try:\n",
    "            bucket = self.get_bucket(bucket_name)\n",
    "            # list_objects = [object for object in bucket.objects.filter(Prefix=filename)]\n",
    "            object_list = list(bucket.objects.filter(Prefix=filename))\n",
    "            file_objs = object_list[0] if len(object_list) == 1 else object_list\n",
    "            logging.info(\"Completed execution the get_file_object method of S3Operations class\")\n",
    "            return file_objs\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def load_model(self, model_name: str, bucket_name: str, model_dir=None) -> object:\n",
    "        \"\"\"\n",
    "        Method Name :   load_model\n",
    "        Description :   This method loads the model_name from bucket_name bucket with kwargs\n",
    "        Output      :   list of objects or object is returned based on filename\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the load_model method of S3Operations class\")\n",
    "\n",
    "        try:\n",
    "            if model_dir is None:\n",
    "                model_file = model_name\n",
    "            else:\n",
    "                model_file = os.path.join(model_dir, model_name)\n",
    "                # model_file = model_dir + \"/\" + model_name\n",
    "\n",
    "            file_object = self.get_file_object(model_file, bucket_name)\n",
    "            model_object = self.read_object(file_object, decode=False)\n",
    "            model = pickle.load(model_object)\n",
    "            logging.info(\"Completed execution of load_model method of S3Operations class\")\n",
    "            return model\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def create_folder(self, folder_name: str, bucket_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Method Name :   create_folder\n",
    "        Description :   This method creates a folder_name folder in bucket_name bucket\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the create_folder method of S3Operations class\")\n",
    "\n",
    "        try:\n",
    "            self.s3_resource.Object(bucket_name, folder_name).load()\n",
    "\n",
    "        except ClientError as error:\n",
    "            if error.response[\"Error\"][\"Code\"] == \"404\":\n",
    "                folder_obj = folder_name + \"/\"\n",
    "                self.s3_client.put_object(Bucket=bucket_name, Key=folder_obj)\n",
    "            else:\n",
    "                pass\n",
    "            logging.info(\"Completed execution of the create_folder method of S3Operations class\")\n",
    "\n",
    "\n",
    "    def upload_file(self, local_file_path: str, file_name: str, bucket_name: str,\n",
    "                    remove: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Method Name :   upload_file\n",
    "        Description :   This method uploads the from_filename file to bucket_name bucket with\n",
    "                        to_filename as bucket filename\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the upload_file method of S3Operations class\")\n",
    "        try:\n",
    "            logging.info(\n",
    "                f\"Uploading {local_file_path} file to {file_name} file in {bucket_name} bucket\"\n",
    "            )\n",
    "\n",
    "            self.s3_resource.Bucket(bucket_name).upload_file(local_file_path, file_name)\n",
    "\n",
    "            logging.info(\n",
    "                f\"Uploaded {local_file_path} file to {file_name} file in {bucket_name} bucket\"\n",
    "            )\n",
    "\n",
    "            if remove is True:\n",
    "                os.remove(local_file_path)\n",
    "                logging.info(f\"Remove is set to {remove}, deleted the file\")\n",
    "            else:\n",
    "                logging.info(f\"Remove is set to {remove}, not deleted the file\")\n",
    "            logging.info(\"Completed execution of the upload_file method of S3Operations class\")\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def upload_folder(self, folder_name: str, bucket_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Method Name :   upload_file\n",
    "        Description :   This method uploads the from_filename file to bucket_name bucket with\n",
    "                        to_filename as bucket filename\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the upload_folder method of S3Operations class\")\n",
    "        try:\n",
    "            folder_list = os.listdir(folder_name)\n",
    "            for file in folder_list:\n",
    "                local_file_path = os.path.join(folder_name, file)\n",
    "                file_name = file\n",
    "                self.upload_file(local_file_path, file_name, bucket_name, remove=False)\n",
    "            logging.info(\"Completed execution of the upload_folder method of S3Operations class\")\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def upload_df_as_csv(self, \n",
    "                         data_frame: DataFrame, \n",
    "                         local_file_path: str, \n",
    "                         bucket_file_name: str, \n",
    "                         bucket_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Method Name :   upload_df_as_csv\n",
    "        Description :   This method uploads the dataframe to bucket_filename csv file\n",
    "                        in bucket_name bucket\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the upload_df_as_csv method of S3Operations class\")\n",
    "        try:\n",
    "            data_frame.to_csv(local_file_path, index=None, header=True)\n",
    "            self.upload_file(local_file_path, bucket_file_name, bucket_name)\n",
    "            logging.info(\"Completed execution of the upload_df_as_csv method of S3Operations class\")\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def get_df_from_object(self, object_: object) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Method Name :   get_df_from_object\n",
    "        Description :   This method gets the dataframe from the object_name object\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the get_df_from_object method of S3Operations class\")\n",
    "\n",
    "        try:\n",
    "            content = self.read_object(object_)\n",
    "            read_csv_df = read_csv(content, na_values=\"na\")\n",
    "            logging.info(\"Completed execution of the get_df_from_object method of S3Operations class\")\n",
    "            return read_csv_df\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n",
    "\n",
    "\n",
    "    def read_csv(self, file_name: str, bucket_name: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Method Name :   get_df_from_object\n",
    "        Description :   This method gets the dataframe from the object_name object\n",
    "        Output      :   Folder is created in s3 bucket\n",
    "        \"\"\"\n",
    "        logging.info(\"Inside the read_csv method of S3Operations class\")\n",
    "        try:\n",
    "            csv_obj = self.get_file_object(file_name, bucket_name)\n",
    "            read_csv_df = self.get_df_from_object(csv_obj)\n",
    "            logging.info(\"Completed execution of the read_csv method of S3Operations class\")\n",
    "            return read_csv_df\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        self.s3_storage = S3Operations()\n",
    "\n",
    "\n",
    "    def get_file_from_url(self):\n",
    "        \"\"\"This method is used to download the file from URL\"\"\"\n",
    "        if not os.path.exists(self.config.downloaded_data_file):\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url = self.config.data_url,\n",
    "                filename = self.config.downloaded_data_file\n",
    "            )\n",
    "            logging.info(f\"{filename} download! with following info: \\n{headers}\")\n",
    "        else:\n",
    "            logging.info(f\"File already exists of size: {get_size(Path(self.config.downloaded_data_file))}\")\n",
    "            logging.info(f\"Pushing the {filename} into Bucket - {self.config.data_bucket_name}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzipped_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.downloaded_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "\n",
    "\n",
    "    def download_data_from_s3(self) -> str:\n",
    "        \"\"\"This method is used to download the data from s3\"\"\"\n",
    "        try:\n",
    "            zip_download_dir = self.config.root_dir\n",
    "            os.makedirs(zip_download_dir, exist_ok=True)\n",
    "\n",
    "            logging.info(f\"Downloading data from s3 into file {zip_download_dir}\")\n",
    "\n",
    "            self.config.downloaded_data_file\n",
    "\n",
    "            self.s3_storage.download_object(\n",
    "                file_name = self.config.data_file_name,\n",
    "                bucket_name = self.config.data_bucket_name,\n",
    "                file_path = self.config.downloaded_data_file,\n",
    "            )\n",
    "\n",
    "            if os.path.exists(self.config.downloaded_data_file):\n",
    "                logging.info(f\"Downloaded data from s3 into file {self.config.downloaded_data_file}\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.exception(error)\n",
    "            raise TextSummarizerException(error, sys) from error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class DataValiadtion:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def validate_all_files_exist(self)-> bool:\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            all_files = os.listdir(DataIngestionConstants.UNZIPPED_DIR)\n",
    "\n",
    "            for file in all_files:\n",
    "                if file not in self.config.all_required_files:\n",
    "                    validation_status = False\n",
    "                    with open(self.config.status_file, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status}\")\n",
    "                else:\n",
    "                    validation_status = True\n",
    "                    with open(self.config.status_file, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status}\")\n",
    "\n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajus\\anaconda3\\envs\\nlp_text_summarizer_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "    def convert_examples_to_features(self, example_batch):\n",
    "        input_encodings = self.tokenizer(example_batch['dialogue'] , \n",
    "                                         max_length = 1024, \n",
    "                                         truncation = True \n",
    "                                        )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(example_batch['summary'], \n",
    "                                              max_length = 128, \n",
    "                                              truncation = True \n",
    "                                            )\n",
    "            \n",
    "        return {\n",
    "            'input_ids' : input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "    \n",
    "\n",
    "    def load_samsum_dataset(self):\n",
    "        data_files = {\n",
    "            \"train\": os.path.join(self.config.transformed_data_path,\"samsum-train.csv\"), \n",
    "            \"validation\": os.path.join(self.config.transformed_data_path ,\"samsum-validation.csv\"), \n",
    "            \"test\": os.path.join(self.config.transformed_data_path ,\"samsum-test.csv\")\n",
    "            }\n",
    "        logging.info(f\"Data files that will be loaded - {data_files}\")\n",
    "\n",
    "        dataset = load_dataset(\"csv\", data_files= data_files)\n",
    "        logging.info(f\" After loading csv in dataset format- {dataset}\")\n",
    "\n",
    "        # Logging the sizes of the datasets\n",
    "        logging.info(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "        logging.info(f\"Text dataset size: {len(dataset['test'])}\")\n",
    "        logging.info(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def preprocess_dataset(self, examples):\n",
    "        prefix = \"Summarize: \"\n",
    "        inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "\n",
    "        model_inputs = self.tokenizer(inputs, \n",
    "                                      max_length= self.config.max_input_length, \n",
    "                                      truncation=True)\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        labels = self.tokenizer(text_target = examples[\"summary\"],\n",
    "                                max_length = self.config.max_target_length, \n",
    "                                truncation=True\n",
    "                                )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def convert(self):\n",
    "        # dataset_samsum = load_from_disk(self.config.transformed_data_path)\n",
    "        dataset_samsum = self.load_samsum_dataset()\n",
    "\n",
    "        logging.info(f\"Logging to asses the dataset - {dataset_samsum}\")\n",
    "\n",
    "        dataset_samsum_pt = dataset_samsum.map(self.preprocess_dataset, batched=True)\n",
    "\n",
    "        logging.info(f\"Logging to asses the dataset after pre-processing - {dataset_samsum_pt}\")\n",
    "\n",
    "        logging.info(f\"Model Inputs for training datasets \\n {dataset_samsum_pt['train'][:2]}\")\n",
    "        logging.info(f\"Model Inputs for test datasets \\n {dataset_samsum_pt['test'][:2]}\")\n",
    "        logging.info(f\"Model Inputs for validation datasets \\n {dataset_samsum_pt['validation'][:2]}\")\n",
    "\n",
    "        dataset_samsum_pt.save_to_disk(self.config.root_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module is used for training model on custom data\"\"\"\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "import os\n",
    "# from accelerate import Accelerator\n",
    "# accelerator = Accelerator()\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    \"\"\"This class encapsulates the method for training the models on custom data\"\"\"\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"This method is used for training the models on custom data\"\"\"\n",
    "        logging.info(\"Inside ModelTraining.train of model_trainer module\")\n",
    "\n",
    "        logging.info(f\"Validating cuda availability - {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"  \n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "        logging.info(f\"Device is set to - {device}\")\n",
    "        \n",
    "        logging.info(\"Setting tokenizer, Model and data collator\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        logging.info(\"Completed setting Tokenizer\")\n",
    "        \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt)\n",
    "        logging.info(\"Completed setting Model\")\n",
    "        \n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\n",
    "        logging.info(\"Completed setting data collator\")\n",
    "\n",
    "        logging.info(f\"Loading data from disk {self.config.data_path}\")\n",
    "        dataset_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        logging.info(dataset_pt)\n",
    "\n",
    "        logging.info(f\"Setting TrainingArguments for model training \")\n",
    "        trainer_args = Seq2SeqTrainingArguments(\n",
    "            output_dir = self.config.root_dir, \n",
    "            num_train_epochs = self.config.num_train_epochs,\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = self.config.per_device_train_batch_size,\n",
    "            weight_decay = self.config.weight_decay, \n",
    "            logging_steps = self.config.logging_steps,\n",
    "            evaluation_strategy = self.config.evaluation_strategy, \n",
    "            eval_steps = self.config.eval_steps, \n",
    "            save_steps = self.config.save_steps,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps\n",
    "        ) \n",
    "\n",
    "        logging.info(f\"Setting Model Parameters for Trainer \")\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model = model, \n",
    "            args=trainer_args,\n",
    "            tokenizer = tokenizer, \n",
    "            data_collator = seq2seq_data_collator,\n",
    "            train_dataset = dataset_pt[\"train\"],\n",
    "            eval_dataset = dataset_pt[\"validation\"]\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"Model Training Started....\")\n",
    "        trainer.train()\n",
    "        logging.info(f\"Model Training completed.\")\n",
    "\n",
    "        logging.info(f\"Saving Trained Model - {self.config.root_dir}\")\n",
    "        model.save_pretrained(os.path.join(self.config.root_dir,\"model\"))\n",
    "\n",
    "        logging.info(f\"Saving tokenizer - {self.config.root_dir}\")\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module is used for Evaluating the models after training\"\"\"\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "    \n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, \n",
    "                               device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "        \n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "            \n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences \n",
    "            that are too long. '''\n",
    "            \n",
    "            # Finally, we decode the generated texts, \n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=True) \n",
    "                for s in summaries]     \n",
    "            \n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            \n",
    "            \n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "            \n",
    "        #  Finally compute and return the ROUGE scores.\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "       \n",
    "        #loading data \n",
    "        dataset_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "  \n",
    "        rouge_metric = load_metric('rouge')\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    "            )\n",
    "\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n",
    "        df.to_csv(self.config.metric_file_name, index=False)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionPipeline:\n",
    "    \"\"\"This class contains the methods that triggers the Data Ingestion Pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"This method triggers the Data Ingestion Pipeline\"\"\"\n",
    "        logging.info(\"Inside DataIngestionPipeline.main of model_training_pipeline module\")\n",
    "        config = ConfigurationManager()\n",
    "        data_ingestion_config = config.get_data_ingestion_config()\n",
    "        data_ingestion = DataIngestion(config = data_ingestion_config)\n",
    "        if data_ingestion.get_file_from_url():\n",
    "            logging.info('Calling download_data_from_s3...')\n",
    "            if data_ingestion.download_data_from_s3():\n",
    "                logging.info('Calling extract_zip_file...')\n",
    "                data_ingestion.extract_zip_file()\n",
    "                logging.info(\"Completed execution of DataIngestionPipeline.main of model_training_pipeline module\")\n",
    "            else:\n",
    "                logging.exception('Failed to download files from S3') \n",
    "        else:\n",
    "            logging.exception('Failed to get files from Open Source')\n",
    "\n",
    "\n",
    "class DataValidationPipeline:\n",
    "    \"\"\"This class contains the methods that triggers the Data Validation Pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"This method triggers the Data Validation Pipeline\"\"\"\n",
    "        logging.info(\"Inside DataValidationPipeline.main of model_training_pipeline module\")\n",
    "        config = ConfigurationManager()\n",
    "        data_validation_config = config.get_data_validation_config()\n",
    "        data_validation = DataValiadtion(config = data_validation_config)\n",
    "        data_validation.validate_all_files_exist()\n",
    "        logging.info(\"Completed execution of DataValidationPipeline.main of model_training_pipeline module\")\n",
    "\n",
    "\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    \"\"\"This class contains the methods that triggers the Data Transformation Pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"This method triggers the Data Transformation Pipeline\"\"\"\n",
    "        logging.info(\"Inside DataTransformationPipeline.main of model_training_pipeline module\")\n",
    "        config = ConfigurationManager()\n",
    "        data_transformation_config = config.get_data_transformation_config()\n",
    "        data_transformation = DataTransformation(config=data_transformation_config)\n",
    "        data_transformation.convert()\n",
    "        logging.info(\"Completed execution of DataTransformationPipeline.main of model_training_pipeline module\")\n",
    "\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    \"\"\"This class contains the methods that triggers the Model Training Pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"This method triggers the Model Training Pipeline\"\"\"\n",
    "        logging.info(\"Inside ModelTrainingPipeline.main of model_training_pipeline module\")\n",
    "        config = ConfigurationManager()\n",
    "        model_trainer_config = config.get_model_training_config()\n",
    "        model_trainer_config = ModelTraining(config = model_trainer_config)\n",
    "        model_trainer_config.train()\n",
    "        logging.info(\"Completed execution of ModelTrainingPipeline.main of model_training_pipeline module\")\n",
    "\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "    \"\"\"This class contains the methods that triggers the Model Evaluation Pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"This method triggers the Model Evaluation Pipeline\"\"\"\n",
    "        logging.info(\"Inside ModelEvaluationPipeline.main of model_training_pipeline module\")\n",
    "        config = ConfigurationManager()\n",
    "        model_evaluation_config = config.get_model_evaluation_config()\n",
    "        model_evaluation_config = ModelEvaluation(config = model_evaluation_config)\n",
    "        model_evaluation_config.evaluate()\n",
    "        logging.info(\"Completed execution of ModelEvaluationPipeline.main of model_training_pipeline module\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class PredictionPipeline:\n",
    "    def __init__(self):\n",
    "        self.config = ConfigurationManager().get_model_evaluation_config()\n",
    "\n",
    "    \n",
    "    def predict(self,text):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "\n",
    "        pipe = pipeline(\"summarization\", model=self.config.model_path,tokenizer=tokenizer)\n",
    "\n",
    "        print(\"Dialogue:\")\n",
    "        print(text)\n",
    "\n",
    "        output = pipe(text, **gen_kwargs)[0][\"summary_text\"]\n",
    "        print(\"\\nModel Summary:\")\n",
    "        print(output)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_text_summarizer_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
